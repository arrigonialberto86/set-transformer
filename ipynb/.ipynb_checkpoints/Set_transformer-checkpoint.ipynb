{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# MultiHeadAttention\n",
    "# https://www.tensorflow.org/tutorials/text/transformer, appears in \"Attention is all you need\" NIPS 2018 paper\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead) \n",
    "    but it must be broadcastable for addition.\n",
    "\n",
    "    Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "    output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "def print_out(q, k, v):\n",
    "    temp_out, temp_attn = scaled_dot_product_attention(\n",
    "      q, k, v, None)\n",
    "    print ('Attention weights are:')\n",
    "    print (temp_attn)\n",
    "    print ('Output is:')\n",
    "    print (temp_out)\n",
    "    \n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "temp_k = tf.constant([[10,0,0],\n",
    "                      [0,10,0],\n",
    "                      [0,0,10],\n",
    "                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n",
    "\n",
    "temp_v = tf.constant([[   1,0],\n",
    "                      [  10,0],\n",
    "                      [ 100,5],\n",
    "                      [1000,6]], dtype=tf.float32)  # (4, 2)\n",
    "\n",
    "# This `query` aligns with the second `key`,\n",
    "# so the second `value` is returned.\n",
    "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "    \n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "    \n",
    "        self.depth = d_model // self.num_heads\n",
    "    \n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, q, k, v, mask=None):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "    \n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output\n",
    "    \n",
    "\n",
    "# temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "# y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
    "# out = temp_mha(v=y, k=y, q=y)\n",
    "# print(out.shape)\n",
    "\n",
    "\n",
    "class RFF(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Row-wise FeedForward layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, d):\n",
    "        super(RFF, self).__init__()\n",
    "        \n",
    "        self.linear_1 = Dense(d, activation='relu')\n",
    "        self.linear_2 = Dense(d, activation='relu')\n",
    "        self.linear_3 = Dense(d, activation='relu')\n",
    "            \n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: a float tensor with shape [b, n, d].\n",
    "        Returns:\n",
    "            a float tensor with shape [b, n, d].\n",
    "        \"\"\"\n",
    "        return self.linear_3(self.linear_2(self.linear_1(x)))   \n",
    "\n",
    "\n",
    "# mlp = RFF(3)\n",
    "# y = mlp(tf.ones(shape=(2, 4, 3)))  # The first call to the `mlp` will create the weights\n",
    "# print('weights:', len(mlp.weights))\n",
    "# print('trainable weights:', len(mlp.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Referencing https://arxiv.org/pdf/1810.00825.pdf \n",
    "# and the original PyTorch implementation https://github.com/TropComplique/set-transformer/blob/master/blocks.py\n",
    "from tensorflow import repeat\n",
    "\n",
    "\n",
    "class MultiHeadAttentionBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, d, h, rff):\n",
    "        super(MultiHeadAttentionBlock, self).__init__()\n",
    "        self.multihead = MultiHeadAttention(d, h)\n",
    "        self.layer_norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.rff = rff\n",
    "    \n",
    "    def call(self, x, y):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: a float tensor with shape [b, n, d].\n",
    "            y: a float tensor with shape [b, m, d].\n",
    "        Returns:\n",
    "            a float tensor with shape [b, n, d].\n",
    "        \"\"\"\n",
    "    \n",
    "        h = self.layer_norm1(x + self.multihead(x, y, y))\n",
    "        return self.layer_norm2(h + self.rff(h))\n",
    "\n",
    "# x_data = tf.random.normal(shape=(10, 2, 9))\n",
    "# y_data = tf.random.normal(shape=(10, 3, 9))\n",
    "# rff = RFF(d=9)\n",
    "# mab = MultiHeadAttentionBlock(9, 3, rff=rff)\n",
    "# mab(x_data, y_data).shape    \n",
    "\n",
    "    \n",
    "class SetAttentionBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, d, h, rff):\n",
    "        super().__init__()\n",
    "        self.mab = MultiHeadAttentionBlock(d, h, rff)\n",
    "    \n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: a float tensor with shape [b, n, d].\n",
    "        Returns:\n",
    "            a float tensor with shape [b, n, d].\n",
    "        \"\"\"\n",
    "        return self.mab(x, x)\n",
    "\n",
    "    \n",
    "class InducedSetAttentionBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, d, m, h, rff1, rff2):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            d: an integer, input dimension.\n",
    "            m: an integer, number of inducing points.\n",
    "            h: an integer, number of heads.\n",
    "            rff1, rff2: modules, row-wise feedforward layers.\n",
    "                It takes a float tensor with shape [b, n, d] and\n",
    "                returns a float tensor with the same shape.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.mab1 = MultiHeadAttentionBlock(d, h, rff1)\n",
    "        self.mab2 = MultiHeadAttentionBlock(d, h, rff2)\n",
    "        self.inducing_points = tf.random.normal(shape=(1, m, d))\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: a float tensor with shape [b, n, d].\n",
    "        Returns:\n",
    "            a float tensor with shape [b, n, d].\n",
    "        \"\"\"\n",
    "        b = x.shape[0]\n",
    "        p = self.inducing_points\n",
    "        p = repeat(p, (b), axis=0)  # shape [b, m, d]  \n",
    "        h = self.mab1(p, x)  # shape [b, m, d]\n",
    "        return self.mab2(x, h)     \n",
    "    \n",
    "\n",
    "class PoolingMultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d, k, h, rff, rff_s):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            d: an integer, input dimension.\n",
    "            k: an integer, number of seed vectors.\n",
    "            h: an integer, number of heads.\n",
    "            rff: a module, row-wise feedforward layers.\n",
    "                It takes a float tensor with shape [b, n, d] and\n",
    "                returns a float tensor with the same shape.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.mab = MultiHeadAttentionBlock(d, h, rff)\n",
    "        self.seed_vectors = tf.random.normal(shape=(1, k, d))\n",
    "        self.rff_s = rff_s\n",
    "\n",
    "    def call(self, z):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            z: a float tensor with shape [b, n, d].\n",
    "        Returns:\n",
    "            a float tensor with shape [b, k, d]\n",
    "        \"\"\"\n",
    "        b = z.shape[0]\n",
    "        s = self.seed_vectors\n",
    "        s = repeat(s, (b), axis=0)  # shape [b, k, d]\n",
    "        return self.mab(s, self.rff_s(z))\n",
    "    \n",
    "\n",
    "# z = tf.random.normal(shape=(10, 2, 9))\n",
    "# rff, rff_s = RFF(d=9), RFF(d=9) \n",
    "# pma = PoolingMultiHeadAttention(d=9, k=10, h=3, rff=rff, rff_s=rff_s)\n",
    "# pma(z).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "class STEncoderBasic(tf.keras.layers.Layer):\n",
    "    def __init__(self, d=12, m=6, h=6):\n",
    "        super(STEncoderBasic, self).__init__()\n",
    "        \n",
    "        # Embedding part\n",
    "        self.linear_1 = Dense(d, activation='relu')\n",
    "        self.isab_1 = InducedSetAttentionBlock(d, m, h, RFF(d), RFF(d))\n",
    "        self.isab_2 = InducedSetAttentionBlock(d, m, h, RFF(d), RFF(d)) \n",
    "            \n",
    "    def call(self, x):\n",
    "        return self.isab_2(self.isab_1(self.linear_1(x)))\n",
    "\n",
    "    \n",
    "class STDecoderBasic(tf.keras.layers.Layer):\n",
    "    def __init__(self, out_dim, d=12, m=6, h=2, k=8):\n",
    "        super(STDecoderBasic, self).__init__()\n",
    "        \n",
    "        self.PMA = PoolingMultiHeadAttention(d, k, h, RFF(d), RFF(d))\n",
    "#         self.SAB = SetAttentionBlock(d, h, RFF(d))\n",
    "#         self.output_mapper = Dense(out_dim)      \n",
    "\n",
    "    def call(self, x):\n",
    "#         return self.output_mapper(self.SAB(self.PMA(x)))\n",
    "        return self.PMA(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 2, 1)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gen_data(batch_size, max_length=30, test=False):\n",
    "    length = np.random.randint(1, max_length + 1)\n",
    "    x = np.random.uniform(1, 100, (batch_size, length))\n",
    "    y = np.max(x, axis=1)\n",
    "    x, y = np.expand_dims(x, axis=2), np.expand_dims(y, axis=1)\n",
    "    return x, y\n",
    "\n",
    "x,y = gen_data(batch_size=9)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer st_encoder_basic_53 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "(9, 2, 1)\n",
      "(9, 2, 1)\n"
     ]
    }
   ],
   "source": [
    "encoder = STEncoderBasic(d=1, m=1, h=1)\n",
    "encoded = encoder(x)\n",
    "print(encoded.shape)\n",
    "\n",
    "decoder = STDecoderBasic(out_dim=1, d=1, m=1, h=1, k=2)\n",
    "decoded = decoder(encoded)\n",
    "print(decoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

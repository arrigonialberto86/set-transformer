{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\",category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# MultiHeadAttention\n",
    "# https://www.tensorflow.org/tutorials/text/transformer, appears in \"Attention is all you need\" NIPS 2018 paper\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead) \n",
    "    but it must be broadcastable for addition.\n",
    "\n",
    "    Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "    output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "def print_out(q, k, v):\n",
    "    temp_out, temp_attn = scaled_dot_product_attention(\n",
    "      q, k, v, None)\n",
    "    print ('Attention weights are:')\n",
    "    print (temp_attn)\n",
    "    print ('Output is:')\n",
    "    print (temp_out)\n",
    "    \n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "temp_k = tf.constant([[10,0,0],\n",
    "                      [0,10,0],\n",
    "                      [0,0,10],\n",
    "                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n",
    "\n",
    "temp_v = tf.constant([[   1,0],\n",
    "                      [  10,0],\n",
    "                      [ 100,5],\n",
    "                      [1000,6]], dtype=tf.float32)  # (4, 2)\n",
    "\n",
    "# This `query` aligns with the second `key`,\n",
    "# so the second `value` is returned.\n",
    "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "    \n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "    \n",
    "        self.depth = d_model // self.num_heads\n",
    "    \n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, q, k, v, mask=None):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "    \n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output\n",
    "    \n",
    "\n",
    "# temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "# y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
    "# out = temp_mha(v=y, k=y, q=y)\n",
    "# print(out.shape)\n",
    "\n",
    "\n",
    "class RFF(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Row-wise FeedForward layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, d):\n",
    "        super(RFF, self).__init__()\n",
    "        \n",
    "        self.linear_1 = Dense(d, activation='relu')\n",
    "        self.linear_2 = Dense(d, activation='relu')\n",
    "        self.linear_3 = Dense(d, activation='relu')\n",
    "            \n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: a float tensor with shape [b, n, d].\n",
    "        Returns:\n",
    "            a float tensor with shape [b, n, d].\n",
    "        \"\"\"\n",
    "        return self.linear_3(self.linear_2(self.linear_1(x)))   \n",
    "\n",
    "\n",
    "# mlp = RFF(3)\n",
    "# y = mlp(tf.ones(shape=(2, 4, 3)))  # The first call to the `mlp` will create the weights\n",
    "# print('weights:', len(mlp.weights))\n",
    "# print('trainable weights:', len(mlp.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Referencing https://arxiv.org/pdf/1810.00825.pdf \n",
    "# and the original PyTorch implementation https://github.com/TropComplique/set-transformer/blob/master/blocks.py\n",
    "from tensorflow import repeat\n",
    "# from tensorflow.keras.backend import repeat_elements\n",
    "from tensorflow.keras.layers import LayerNormalization\n",
    "\n",
    "\n",
    "class MultiHeadAttentionBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, d, h, rff):\n",
    "        super(MultiHeadAttentionBlock, self).__init__()\n",
    "        self.multihead = MultiHeadAttention(d, h)\n",
    "        self.layer_norm1 = LayerNormalization(epsilon=1e-6, dtype='float32')\n",
    "        self.layer_norm2 = LayerNormalization(epsilon=1e-6, dtype='float32')\n",
    "        self.rff = rff\n",
    "    \n",
    "    def call(self, x, y):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: a float tensor with shape [b, n, d].\n",
    "            y: a float tensor with shape [b, m, d].\n",
    "        Returns:\n",
    "            a float tensor with shape [b, n, d].\n",
    "        \"\"\"\n",
    "    \n",
    "        h = self.layer_norm1(x + self.multihead(x, y, y))\n",
    "        return self.layer_norm2(h + self.rff(h))\n",
    "\n",
    "# x_data = tf.random.normal(shape=(10, 2, 9))\n",
    "# y_data = tf.random.normal(shape=(10, 3, 9))\n",
    "# rff = RFF(d=9)\n",
    "# mab = MultiHeadAttentionBlock(9, 3, rff=rff)\n",
    "# mab(x_data, y_data).shape    \n",
    "\n",
    "    \n",
    "class SetAttentionBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, d, h, rff):\n",
    "        super(SetAttentionBlock, self).__init__()\n",
    "        self.mab = MultiHeadAttentionBlock(d, h, rff)\n",
    "    \n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: a float tensor with shape [b, n, d].\n",
    "        Returns:\n",
    "            a float tensor with shape [b, n, d].\n",
    "        \"\"\"\n",
    "        return self.mab(x, x)\n",
    "\n",
    "    \n",
    "class InducedSetAttentionBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, d, m, h, rff1, rff2):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            d: an integer, input dimension.\n",
    "            m: an integer, number of inducing points.\n",
    "            h: an integer, number of heads.\n",
    "            rff1, rff2: modules, row-wise feedforward layers.\n",
    "                It takes a float tensor with shape [b, n, d] and\n",
    "                returns a float tensor with the same shape.\n",
    "        \"\"\"\n",
    "        super(InducedSetAttentionBlock, self).__init__()\n",
    "        self.mab1 = MultiHeadAttentionBlock(d, h, rff1)\n",
    "        self.mab2 = MultiHeadAttentionBlock(d, h, rff2)\n",
    "        self.inducing_points = tf.random.normal(shape=(1, m, d))\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: a float tensor with shape [b, n, d].\n",
    "        Returns:\n",
    "            a float tensor with shape [b, n, d].\n",
    "        \"\"\"\n",
    "        b = tf.shape(x)[0] \n",
    "        p = self.inducing_points\n",
    "        p = repeat(p, (b), axis=0)  # shape [b, m, d]  \n",
    "        \n",
    "        h = self.mab1(p, x)  # shape [b, m, d]\n",
    "        return self.mab2(x, h)     \n",
    "    \n",
    "\n",
    "class PoolingMultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d, k, h, rff, rff_s):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            d: an integer, input dimension.\n",
    "            k: an integer, number of seed vectors.\n",
    "            h: an integer, number of heads.\n",
    "            rff: a module, row-wise feedforward layers.\n",
    "                It takes a float tensor with shape [b, n, d] and\n",
    "                returns a float tensor with the same shape.\n",
    "        \"\"\"\n",
    "        super(PoolingMultiHeadAttention, self).__init__()\n",
    "        self.mab = MultiHeadAttentionBlock(d, h, rff)\n",
    "        self.seed_vectors = tf.random.normal(shape=(1, k, d))\n",
    "        self.rff_s = rff_s\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, z):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            z: a float tensor with shape [b, n, d].\n",
    "        Returns:\n",
    "            a float tensor with shape [b, k, d]\n",
    "        \"\"\"\n",
    "        b = tf.shape(z)[0]\n",
    "        s = self.seed_vectors\n",
    "        s = repeat(s, (b), axis=0)  # shape [b, k, d]\n",
    "        return self.mab(s, self.rff_s(z))\n",
    "    \n",
    "\n",
    "# z = tf.random.normal(shape=(10, 2, 9))\n",
    "# rff, rff_s = RFF(d=9), RFF(d=9) \n",
    "# pma = PoolingMultiHeadAttention(d=9, k=10, h=3, rff=rff, rff_s=rff_s)\n",
    "# pma(z).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "    \n",
    "\n",
    "class STEncoderBasic(tf.keras.layers.Layer):\n",
    "    def __init__(self, d=12, m=6, h=6):\n",
    "        super(STEncoderBasic, self).__init__()\n",
    "        \n",
    "        # Embedding part\n",
    "        self.linear_1 = Dense(d, activation='relu')\n",
    "        \n",
    "        # Encoding part\n",
    "        self.isab_1 = InducedSetAttentionBlock(d, m, h, RFF(d), RFF(d))\n",
    "        self.isab_2 = InducedSetAttentionBlock(d, m, h, RFF(d), RFF(d))\n",
    "            \n",
    "    def call(self, x):\n",
    "        return self.isab_2(self.isab_1(self.linear_1(x)))\n",
    "\n",
    "    \n",
    "class STDecoderBasic(tf.keras.layers.Layer):\n",
    "    def __init__(self, out_dim, d=12, m=6, h=2, k=8):\n",
    "        super(STDecoderBasic, self).__init__()\n",
    "        \n",
    "        self.PMA = PoolingMultiHeadAttention(d, k, h, RFF(d), RFF(d))\n",
    "        self.SAB = SetAttentionBlock(d, h, RFF(d))\n",
    "        self.output_mapper = Dense(out_dim)   \n",
    "        self.k, self.d = k, d\n",
    "\n",
    "    def call(self, x):\n",
    "        decoded_vec = self.SAB(self.PMA(x))\n",
    "        decoded_vec = tf.reshape(decoded_vec, [-1, self.k * self.d])\n",
    "        return tf.reshape(self.output_mapper(decoded_vec), (tf.shape(decoded_vec)[0],))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100000, 9, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gen_max_dataset(dataset_size=100000, set_size=9):\n",
    "    \"\"\"\n",
    "    The number of objects per set is constant in this toy example\n",
    "    \"\"\"\n",
    "    x = np.random.uniform(1, 100, (dataset_size, set_size))\n",
    "    y = np.max(x, axis=1)\n",
    "    x, y = np.expand_dims(x, axis=2), np.expand_dims(y, axis=1)\n",
    "    return tf.cast(x, 'float32'), tf.cast(y, 'float32')\n",
    "\n",
    "X, y = gen_max_dataset()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 9, 3)\n",
      "(100000,)\n"
     ]
    }
   ],
   "source": [
    "# Dimensionality check on encoder-decoder couple\n",
    "\n",
    "encoder = STEncoderBasic(d=3, m=2, h=1)\n",
    "encoded = encoder(X)\n",
    "print(encoded.shape)\n",
    "\n",
    "decoder = STDecoderBasic(out_dim=1, d=1, m=2, h=1, k=1)\n",
    "decoded = decoder(encoded)\n",
    "print(decoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual model for max-set prediction\n",
    "\n",
    "class SetTransformer(tf.keras.Model):\n",
    "    def __init__(self, ):\n",
    "        super(SetTransformer, self).__init__()\n",
    "        self.basic_encoder = STEncoderBasic(d=4, m=3, h=2)\n",
    "        self.basic_decoder = STDecoderBasic(out_dim=1, d=4, m=2, h=2, k=2)\n",
    "    \n",
    "    def call(self, x):\n",
    "        enc_output = self.basic_encoder(x)  # (batch_size, set_len, d_model)\n",
    "        return self.basic_decoder(enc_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100000 samples\n",
      "Epoch 1/6\n",
      "100000/100000 [==============================] - 24s 237us/sample - loss: 29.9259\n",
      "Epoch 2/6\n",
      "100000/100000 [==============================] - 21s 206us/sample - loss: 2.5411\n",
      "Epoch 3/6\n",
      "100000/100000 [==============================] - 20s 199us/sample - loss: 0.5547\n",
      "Epoch 4/6\n",
      "100000/100000 [==============================] - 20s 199us/sample - loss: 0.4607\n",
      "Epoch 5/6\n",
      "100000/100000 [==============================] - 20s 199us/sample - loss: 0.4181\n",
      "Epoch 6/6\n",
      "100000/100000 [==============================] - 21s 205us/sample - loss: 0.4109\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc6943ca978>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_transformer = SetTransformer()\n",
    "set_transformer.compile(loss='mae', optimizer='adam')\n",
    "set_transformer.fit(X, y, epochs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "\n",
    "def extract_image_set(x_data: np.array, y_data :np.array, agg_fun=np.sum, n_images=3) -> Tuple[np.array, np.array]:\n",
    "    \"\"\"\n",
    "    Extract a single set of images with corresponding target\n",
    "    :param x_data\n",
    "    \"\"\"\n",
    "    idxs = np.random.randint(low=0, high=len(x_data)-1, size=n_images)\n",
    "    return x_data[idxs], agg_fun(y_data[idxs])\n",
    "\n",
    "\n",
    "def generate_dataset(n_samples: int, x_data: np.array, y_data :np.array, agg_fun=np.sum, n_images=3) -> Tuple[List[List[np.array]], np.array]:\n",
    "    \"\"\"\n",
    "    :return X,y in format suitable for training/prediction \n",
    "    \"\"\"\n",
    "    generated_list = [extract_image_set(x_data, y_data, agg_fun, n_images) for i in range(n_samples)]\n",
    "    X, y = [i[0] for i in generated_list], np.array([t[1] for t in generated_list])\n",
    "    output_lists = [[] for i in range(n_images)]\n",
    "    for image_idx in range(n_images):\n",
    "        for sample_idx in range(n_samples):\n",
    "            output_lists[image_idx].append(np.expand_dims(X[sample_idx][image_idx], axis=2))\n",
    "    return output_lists, y\n",
    "\n",
    "X_train_data, y_train_data = generate_dataset(n_samples=100000, x_data=x_train, y_data=y_train, n_images=3)\n",
    "X_test_data, y_test_data = generate_dataset(n_samples=20000, x_data=x_test, y_data=y_test, n_images=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x648 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "n_sample_viz, n_images = 3, 3\n",
    "\n",
    "fig, axes = plt.subplots(nrows=n_sample_viz, ncols=n_images, figsize=(9.0, 9.0))\n",
    "\n",
    "for sample_idx in range(n_sample_viz):\n",
    "    for im_idx in range(n_images):\n",
    "        axes[sample_idx, im_idx].imshow(X_train_data[im_idx][sample_idx][:, :, 0], cmap='Greys')\n",
    "        axes[sample_idx, im_idx].axis('off')\n",
    "        if im_idx==0:\n",
    "            axes[sample_idx, 0].set_title('               Sum value for this row is {}'.format(y_train_data[sample_idx]), \n",
    "                                    fontsize=15, loc='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 77s 1ms/sample - loss: 1.0678\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 70s 1ms/sample - loss: 0.5569\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 71s 1ms/sample - loss: 0.4641\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc6a44f0da0>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, define the vision modules\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Add\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "filters = 64\n",
    "kernel_size = 3\n",
    "\n",
    "import tensorflow as tf\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = [np.expand_dims(t, axis=2) for t in x_train]\n",
    "x_test = [np.expand_dims(t, axis=2) for t in x_test]\n",
    "\n",
    "input_image = Input(shape=(28, 28, 1))\n",
    "\n",
    "y = Conv2D(32, kernel_size=(3, 3),\n",
    "           activation='relu',\n",
    "           input_shape=input_shape)(input_image)\n",
    "y = Conv2D(64, (3, 3), activation='relu')(y)\n",
    "y = MaxPooling2D(pool_size=(2, 2))(y)\n",
    "y = Dropout(0.25)(y)\n",
    "y = Flatten()(y)\n",
    "y = Dense(32, activation='relu')(y)\n",
    "y = Dense(16, activation='relu')(y)\n",
    "output_vec = Dense(1)(y)\n",
    "\n",
    "vision_model = Model(input_image, output_vec)\n",
    "vision_model.compile(loss='mae')\n",
    "vision_model.fit(np.array(x_train), np.array(y_train), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vision_model.save('vision_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7581c681606f4d65ba471a048804c4a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=964.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# Get list of different character paths\n",
    "img_dir = './images_background'\n",
    "alphabet_names = [a for a in os.listdir(img_dir) if a[0] != '.'] # get folder names\n",
    "char_paths = []\n",
    "for lang in alphabet_names:\n",
    "    for char in [a for a in os.listdir(img_dir+'/'+lang) if a[0] != '.']:\n",
    "        char_paths.append(img_dir+'/'+lang+'/'+char)\n",
    "\n",
    "char_to_png = {char_path: os.listdir(char_path) for char_path in tqdm(char_paths)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def draw_one_sample(char_paths: List, sample_size=6):\n",
    "    n_chars = np.random.randint(low=1, high=sample_size, size=1)\n",
    "    selected_chars = np.random.choice(char_paths, size=n_chars, replace=False)\n",
    "    rep_char_list = selected_chars.tolist() + \\\n",
    "                    np.random.choice(selected_chars, size=sample_size-len(selected_chars), replace=True).tolist()\n",
    "    sampled_paths = [char_path+'/'+np.random.choice(char_to_png[char_path]) for char_path in rep_char_list]\n",
    "    return sampled_paths, n_chars[0]\n",
    "\n",
    "sampled_paths, n_chars = draw_one_sample(char_paths, sample_size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of selected characters is 4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.figure import Figure\n",
    "from numpy import ndarray\n",
    "from typing import List\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def render_chart(fig: Figure, axis: ndarray, image_id: int, data_list: List):\n",
    "    image = mpimg.imread(data_list[image_id])\n",
    "    axis.title.set_text(data_list[image_id].split('/')[-2])\n",
    "    axis.axis('off')\n",
    "    axis.imshow(image, cmap='gray')\n",
    "\n",
    "print('Number of selected characters is {}'.format(n_chars))    \n",
    "\n",
    "fig, axs = plt.subplots(2, 3)\n",
    "render_chart(fig=fig, axis=axs[0, 0], image_id=0, data_list=sampled_paths)\n",
    "render_chart(fig=fig, axis=axs[0, 1], image_id=1, data_list=sampled_paths)\n",
    "render_chart(fig=fig, axis=axs[0, 2], image_id=2, data_list=sampled_paths)\n",
    "render_chart(fig=fig, axis=axs[1, 0], image_id=3, data_list=sampled_paths)\n",
    "render_chart(fig=fig, axis=axs[1, 1], image_id=4, data_list=sampled_paths)\n",
    "render_chart(fig=fig, axis=axs[1, 2], image_id=5, data_list=sampled_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59fff385c8aa44cf8f9b61383551fdfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=60000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "872f05b8d043413dadfbe238bbd33fb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_size, test_size = 60000, 15000\n",
    "\n",
    "train_dataset = [draw_one_sample(char_paths, sample_size=6) for i in tqdm(range(train_size))]\n",
    "train_X, train_y = [i[0] for i in train_dataset], [i[1] for i in train_dataset]\n",
    "\n",
    "test_dataset = [draw_one_sample(char_paths, sample_size=6) for i in tqdm(range(test_size))]\n",
    "test_X, test_y = [i[0] for i in test_dataset], [i[1] for i in test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from typing import List\n",
    "from tensorflow import convert_to_tensor\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def load_image(file_path):\n",
    "    image = tf.io.read_file(file_path)\n",
    "    return tf.image.decode_png(image, channels=1)\n",
    "\n",
    "@tf.function\n",
    "def load_image_list(image_list: tf.Tensor):\n",
    "    return tf.cast(tf.map_fn(lambda x: load_image(x), image_list, dtype=tf.uint8), tf.float32)\n",
    "\n",
    "\n",
    "class SetDataGenerator:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y \n",
    "        self.dataset = None\n",
    "\n",
    "    def generator_init(self, shuffle_buffer_size=500, repeat=-1, batch_size=64):    \n",
    "        \"\"\"\n",
    "        :param repeat, -1 is the default behaviour to repeat the dataset indefinitely\n",
    "        \"\"\"\n",
    "        self.dataset = tf.data.Dataset.from_tensor_slices((self.X, self.y))\n",
    "        self.dataset = self.dataset.map(lambda x, y: (load_image_list(x), tf.cast(y, tf.float32)))\n",
    "\n",
    "        if shuffle_buffer_size == 0:\n",
    "            self.dataset = self.dataset.repeat(repeat).batch(batch_size).prefetch(buffer_size=AUTOTUNE)\n",
    "        else:\n",
    "            self.dataset = self.dataset.shuffle(shuffle_buffer_size).repeat(repeat).batch(batch_size) \\\n",
    "                .prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    @property\n",
    "    def batch(self):\n",
    "        return self.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique characters is : 5.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-0.5, 104.5, 104.5, -0.5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test data generator\n",
    "\n",
    "set_data_gen = SetDataGenerator(train_X, train_y)\n",
    "set_data_gen.generator_init(batch_size=3)\n",
    "batch_data = next(iter(set_data_gen.batch))\n",
    "\n",
    "im_to_plot = batch_data[0][0].numpy()\n",
    "\n",
    "print('Number of unique characters is : {}'.format(batch_data[1][0].numpy()))\n",
    "\n",
    "fig, axs = plt.subplots(2, 3)\n",
    "axs[0, 0].imshow(im_to_plot[0, :, :, 0], cmap='gray')\n",
    "axs[0, 0].axis('off')\n",
    "axs[0, 1].imshow(im_to_plot[1, :, :, 0], cmap='gray')\n",
    "axs[0, 1].axis('off')\n",
    "axs[0, 2].imshow(im_to_plot[2, :, :, 0], cmap='gray')\n",
    "axs[0, 2].axis('off')\n",
    "axs[1, 0].imshow(im_to_plot[3, :, :, 0], cmap='gray')\n",
    "axs[1, 0].axis('off')\n",
    "axs[1, 1].imshow(im_to_plot[4, :, :, 0], cmap='gray')\n",
    "axs[1, 1].axis('off')\n",
    "axs[1, 2].imshow(im_to_plot[5, :, :, 0], cmap='gray')\n",
    "axs[1, 2].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3 samples\n",
      "Epoch 1/3\n",
      "3/3 [==============================] - 7s 2s/sample - loss: 4.7445\n",
      "Epoch 2/3\n",
      "3/3 [==============================] - 2s 601ms/sample - loss: 3.4197\n",
      "Epoch 3/3\n",
      "3/3 [==============================] - 2s 654ms/sample - loss: 1.0533\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f02d5f6d320>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LayerNormalization, Dense\n",
    "import tensorflow as tf\n",
    "from set_transformer.layers.attention import MultiHeadAttention\n",
    "from set_transformer.layers import RFF\n",
    "from set_transformer.blocks import SetAttentionBlock, PoolingMultiHeadAttention\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Conv2D, Input, MaxPooling2D, Dropout, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "\n",
    "def image_processing_model(input_image_shape = (105, 105, 1), output_len=128):\n",
    "    input_image = Input(shape=input_image_shape)\n",
    "    y = Conv2D(64, kernel_size=(3, 3),\n",
    "               activation='relu',\n",
    "               input_shape=input_image_shape)(input_image)\n",
    "    y = Conv2D(64, (3, 3), activation='relu')(y)\n",
    "    y = Conv2D(64, (3, 3), activation='relu')(y)\n",
    "    y = Conv2D(64, (3, 3), activation='relu')(y)\n",
    "    y = MaxPooling2D(pool_size=(2, 2))(y)\n",
    "    y = Dropout(0.25)(y)\n",
    "    y = Flatten()(y)\n",
    "    output_vec = Dense(output_len, activation='relu')(y)\n",
    "    return Model(input_image, output_vec)\n",
    "\n",
    "\n",
    "class CharEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, d=128, h=8):\n",
    "        super(CharEncoder, self).__init__()\n",
    "\n",
    "        # Instantiate image processing model\n",
    "        self.image_model = image_processing_model(output_len=d)\n",
    "\n",
    "        # Encoding part\n",
    "        self.sab_1 = SetAttentionBlock(d, h, RFF(d))\n",
    "        self.sab_2 = SetAttentionBlock(d, h, RFF(d))\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.sab_2(self.sab_1(tf.map_fn(self.image_model, x)))\n",
    "    \n",
    "\n",
    "class CharDecoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, out_dim, d=128, h=8, k=32):\n",
    "        super(CharDecoder, self).__init__()\n",
    "\n",
    "        self.PMA = PoolingMultiHeadAttention(d, k, h, RFF(d), RFF(d))\n",
    "        self.SAB = SetAttentionBlock(d, h, RFF(d))\n",
    "        self.output_mapper = Dense(out_dim)\n",
    "        self.k, self.d = k, d\n",
    "\n",
    "    def call(self, x):\n",
    "        decoded_vec = self.SAB(self.PMA(x))\n",
    "        decoded_vec = tf.reshape(decoded_vec, [-1, self.k * self.d])\n",
    "        return tf.reshape(self.output_mapper(decoded_vec), (tf.shape(decoded_vec)[0],))\n",
    "\n",
    "    \n",
    "class CharSetTransformer(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CharSetTransformer, self).__init__()\n",
    "        self.encoder = CharEncoder()\n",
    "        self.decoder = CharDecoder(out_dim=1)\n",
    "\n",
    "    def call(self, x):\n",
    "        enc_output = self.encoder(x)  # (batch_size, set_len, d_model)\n",
    "        return self.decoder(enc_output)\n",
    "    \n",
    "\n",
    "tset_model = CharSetTransformer()\n",
    "tset_model.compile(loss='mae', optimizer='adam')\n",
    "tset_model.fit(batch_data[0], batch_data[1], epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_data_gen = SetDataGenerator(train_X, train_y)\n",
    "# set_data_gen.generator_init(batch_size=64)\n",
    "\n",
    "# tset_model = CharSetTransformer()\n",
    "# tset_model.compile(loss='mae', optimizer='adam')\n",
    "# tset_model.fit(set_data_gen.batch, epochs=3, steps_per_epoch=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

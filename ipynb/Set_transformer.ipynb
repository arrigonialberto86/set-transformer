{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\",category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# MultiHeadAttention\n",
    "# https://www.tensorflow.org/tutorials/text/transformer, appears in \"Attention is all you need\" NIPS 2018 paper\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead) \n",
    "    but it must be broadcastable for addition.\n",
    "\n",
    "    Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "    output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "def print_out(q, k, v):\n",
    "    temp_out, temp_attn = scaled_dot_product_attention(\n",
    "      q, k, v, None)\n",
    "    print ('Attention weights are:')\n",
    "    print (temp_attn)\n",
    "    print ('Output is:')\n",
    "    print (temp_out)\n",
    "    \n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "temp_k = tf.constant([[10,0,0],\n",
    "                      [0,10,0],\n",
    "                      [0,0,10],\n",
    "                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n",
    "\n",
    "temp_v = tf.constant([[   1,0],\n",
    "                      [  10,0],\n",
    "                      [ 100,5],\n",
    "                      [1000,6]], dtype=tf.float32)  # (4, 2)\n",
    "\n",
    "# This `query` aligns with the second `key`,\n",
    "# so the second `value` is returned.\n",
    "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "    \n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "    \n",
    "        self.depth = d_model // self.num_heads\n",
    "    \n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, q, k, v, mask=None):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "    \n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output\n",
    "    \n",
    "\n",
    "# temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "# y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
    "# out = temp_mha(v=y, k=y, q=y)\n",
    "# print(out.shape)\n",
    "\n",
    "\n",
    "class RFF(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Row-wise FeedForward layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, d):\n",
    "        super(RFF, self).__init__()\n",
    "        \n",
    "        self.linear_1 = Dense(d, activation='relu')\n",
    "        self.linear_2 = Dense(d, activation='relu')\n",
    "        self.linear_3 = Dense(d, activation='relu')\n",
    "            \n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: a float tensor with shape [b, n, d].\n",
    "        Returns:\n",
    "            a float tensor with shape [b, n, d].\n",
    "        \"\"\"\n",
    "        return self.linear_3(self.linear_2(self.linear_1(x)))   \n",
    "\n",
    "\n",
    "# mlp = RFF(3)\n",
    "# y = mlp(tf.ones(shape=(2, 4, 3)))  # The first call to the `mlp` will create the weights\n",
    "# print('weights:', len(mlp.weights))\n",
    "# print('trainable weights:', len(mlp.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Referencing https://arxiv.org/pdf/1810.00825.pdf \n",
    "# and the original PyTorch implementation https://github.com/TropComplique/set-transformer/blob/master/blocks.py\n",
    "from tensorflow import repeat\n",
    "# from tensorflow.keras.backend import repeat_elements\n",
    "from tensorflow.keras.layers import LayerNormalization\n",
    "\n",
    "\n",
    "class MultiHeadAttentionBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, d, h, rff):\n",
    "        super(MultiHeadAttentionBlock, self).__init__()\n",
    "        self.multihead = MultiHeadAttention(d, h)\n",
    "        self.layer_norm1 = LayerNormalization(epsilon=1e-6, dtype='float32')\n",
    "        self.layer_norm2 = LayerNormalization(epsilon=1e-6, dtype='float32')\n",
    "        self.rff = rff\n",
    "    \n",
    "    def call(self, x, y):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: a float tensor with shape [b, n, d].\n",
    "            y: a float tensor with shape [b, m, d].\n",
    "        Returns:\n",
    "            a float tensor with shape [b, n, d].\n",
    "        \"\"\"\n",
    "    \n",
    "        h = self.layer_norm1(x + self.multihead(x, y, y))\n",
    "        return self.layer_norm2(h + self.rff(h))\n",
    "\n",
    "# x_data = tf.random.normal(shape=(10, 2, 9))\n",
    "# y_data = tf.random.normal(shape=(10, 3, 9))\n",
    "# rff = RFF(d=9)\n",
    "# mab = MultiHeadAttentionBlock(9, 3, rff=rff)\n",
    "# mab(x_data, y_data).shape    \n",
    "\n",
    "    \n",
    "class SetAttentionBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, d, h, rff):\n",
    "        super(SetAttentionBlock, self).__init__()\n",
    "        self.mab = MultiHeadAttentionBlock(d, h, rff)\n",
    "    \n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: a float tensor with shape [b, n, d].\n",
    "        Returns:\n",
    "            a float tensor with shape [b, n, d].\n",
    "        \"\"\"\n",
    "        return self.mab(x, x)\n",
    "\n",
    "    \n",
    "class InducedSetAttentionBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, d, m, h, rff1, rff2):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            d: an integer, input dimension.\n",
    "            m: an integer, number of inducing points.\n",
    "            h: an integer, number of heads.\n",
    "            rff1, rff2: modules, row-wise feedforward layers.\n",
    "                It takes a float tensor with shape [b, n, d] and\n",
    "                returns a float tensor with the same shape.\n",
    "        \"\"\"\n",
    "        super(InducedSetAttentionBlock, self).__init__()\n",
    "        self.mab1 = MultiHeadAttentionBlock(d, h, rff1)\n",
    "        self.mab2 = MultiHeadAttentionBlock(d, h, rff2)\n",
    "        self.inducing_points = tf.random.normal(shape=(1, m, d))\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: a float tensor with shape [b, n, d].\n",
    "        Returns:\n",
    "            a float tensor with shape [b, n, d].\n",
    "        \"\"\"\n",
    "        b = tf.shape(x)[0] \n",
    "        p = self.inducing_points\n",
    "        p = repeat(p, (b), axis=0)  # shape [b, m, d]  \n",
    "        \n",
    "        h = self.mab1(p, x)  # shape [b, m, d]\n",
    "        return self.mab2(x, h)     \n",
    "    \n",
    "\n",
    "class PoolingMultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d, k, h, rff, rff_s):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            d: an integer, input dimension.\n",
    "            k: an integer, number of seed vectors.\n",
    "            h: an integer, number of heads.\n",
    "            rff: a module, row-wise feedforward layers.\n",
    "                It takes a float tensor with shape [b, n, d] and\n",
    "                returns a float tensor with the same shape.\n",
    "        \"\"\"\n",
    "        super(PoolingMultiHeadAttention, self).__init__()\n",
    "        self.mab = MultiHeadAttentionBlock(d, h, rff)\n",
    "        self.seed_vectors = tf.random.normal(shape=(1, k, d))\n",
    "        self.rff_s = rff_s\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, z):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            z: a float tensor with shape [b, n, d].\n",
    "        Returns:\n",
    "            a float tensor with shape [b, k, d]\n",
    "        \"\"\"\n",
    "        b = tf.shape(z)[0]\n",
    "        s = self.seed_vectors\n",
    "        s = repeat(s, (b), axis=0)  # shape [b, k, d]\n",
    "        return self.mab(s, self.rff_s(z))\n",
    "    \n",
    "\n",
    "# z = tf.random.normal(shape=(10, 2, 9))\n",
    "# rff, rff_s = RFF(d=9), RFF(d=9) \n",
    "# pma = PoolingMultiHeadAttention(d=9, k=10, h=3, rff=rff, rff_s=rff_s)\n",
    "# pma(z).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "    \n",
    "\n",
    "class STEncoderBasic(tf.keras.layers.Layer):\n",
    "    def __init__(self, d=12, m=6, h=6):\n",
    "        super(STEncoderBasic, self).__init__()\n",
    "        \n",
    "        # Embedding part\n",
    "        self.linear_1 = Dense(d, activation='relu')\n",
    "        \n",
    "        # Encoding part\n",
    "        self.isab_1 = InducedSetAttentionBlock(d, m, h, RFF(d), RFF(d))\n",
    "        self.isab_2 = InducedSetAttentionBlock(d, m, h, RFF(d), RFF(d))\n",
    "            \n",
    "    def call(self, x):\n",
    "        return self.isab_2(self.isab_1(self.linear_1(x)))\n",
    "\n",
    "    \n",
    "class STDecoderBasic(tf.keras.layers.Layer):\n",
    "    def __init__(self, out_dim, d=12, m=6, h=2, k=8):\n",
    "        super(STDecoderBasic, self).__init__()\n",
    "        \n",
    "        self.PMA = PoolingMultiHeadAttention(d, k, h, RFF(d), RFF(d))\n",
    "        self.SAB = SetAttentionBlock(d, h, RFF(d))\n",
    "        self.output_mapper = Dense(out_dim)   \n",
    "        self.k, self.d = k, d\n",
    "\n",
    "    def call(self, x):\n",
    "        decoded_vec = self.SAB(self.PMA(x))\n",
    "        decoded_vec = tf.reshape(decoded_vec, [-1, self.k * self.d])\n",
    "        return tf.reshape(self.output_mapper(decoded_vec), (tf.shape(decoded_vec)[0],))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100000, 9, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gen_max_dataset(dataset_size=100000, set_size=9):\n",
    "    \"\"\"\n",
    "    The number of objects per set is constant in this toy example\n",
    "    \"\"\"\n",
    "    x = np.random.uniform(1, 100, (dataset_size, set_size))\n",
    "    y = np.max(x, axis=1)\n",
    "    x, y = np.expand_dims(x, axis=2), np.expand_dims(y, axis=1)\n",
    "    return tf.cast(x, 'float32'), tf.cast(y, 'float32')\n",
    "\n",
    "X, y = gen_max_dataset()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 9, 3)\n",
      "(100000,)\n"
     ]
    }
   ],
   "source": [
    "# Dimensionality check on encoder-decoder couple\n",
    "\n",
    "encoder = STEncoderBasic(d=3, m=2, h=1)\n",
    "encoded = encoder(X)\n",
    "print(encoded.shape)\n",
    "\n",
    "decoder = STDecoderBasic(out_dim=1, d=1, m=2, h=1, k=1)\n",
    "decoded = decoder(encoded)\n",
    "print(decoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual model for max-set prediction\n",
    "\n",
    "class SetTransformer(tf.keras.Model):\n",
    "    def __init__(self, ):\n",
    "        super(SetTransformer, self).__init__()\n",
    "        self.basic_encoder = STEncoderBasic(d=4, m=3, h=2)\n",
    "        self.basic_decoder = STDecoderBasic(out_dim=1, d=4, m=2, h=2, k=2)\n",
    "    \n",
    "    def call(self, x):\n",
    "        enc_output = self.basic_encoder(x)  # (batch_size, set_len, d_model)\n",
    "        return self.basic_decoder(enc_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100000 samples\n",
      "Epoch 1/6\n",
      "100000/100000 [==============================] - 24s 237us/sample - loss: 29.9259\n",
      "Epoch 2/6\n",
      "100000/100000 [==============================] - 21s 206us/sample - loss: 2.5411\n",
      "Epoch 3/6\n",
      "100000/100000 [==============================] - 20s 199us/sample - loss: 0.5547\n",
      "Epoch 4/6\n",
      "100000/100000 [==============================] - 20s 199us/sample - loss: 0.4607\n",
      "Epoch 5/6\n",
      "100000/100000 [==============================] - 20s 199us/sample - loss: 0.4181\n",
      "Epoch 6/6\n",
      "100000/100000 [==============================] - 21s 205us/sample - loss: 0.4109\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc6943ca978>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_transformer = SetTransformer()\n",
    "set_transformer.compile(loss='mae', optimizer='adam')\n",
    "set_transformer.fit(X, y, epochs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "\n",
    "def extract_image_set(x_data: np.array, y_data :np.array, agg_fun=np.sum, n_images=3) -> Tuple[np.array, np.array]:\n",
    "    \"\"\"\n",
    "    Extract a single set of images with corresponding target\n",
    "    :param x_data\n",
    "    \"\"\"\n",
    "    idxs = np.random.randint(low=0, high=len(x_data)-1, size=n_images)\n",
    "    return x_data[idxs], agg_fun(y_data[idxs])\n",
    "\n",
    "\n",
    "def generate_dataset(n_samples: int, x_data: np.array, y_data :np.array, agg_fun=np.sum, n_images=3) -> Tuple[List[List[np.array]], np.array]:\n",
    "    \"\"\"\n",
    "    :return X,y in format suitable for training/prediction \n",
    "    \"\"\"\n",
    "    generated_list = [extract_image_set(x_data, y_data, agg_fun, n_images) for i in range(n_samples)]\n",
    "    X, y = [i[0] for i in generated_list], np.array([t[1] for t in generated_list])\n",
    "    output_lists = [[] for i in range(n_images)]\n",
    "    for image_idx in range(n_images):\n",
    "        for sample_idx in range(n_samples):\n",
    "            output_lists[image_idx].append(np.expand_dims(X[sample_idx][image_idx], axis=2))\n",
    "    return output_lists, y\n",
    "\n",
    "X_train_data, y_train_data = generate_dataset(n_samples=100000, x_data=x_train, y_data=y_train, n_images=3)\n",
    "X_test_data, y_test_data = generate_dataset(n_samples=20000, x_data=x_test, y_data=y_test, n_images=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x648 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "n_sample_viz, n_images = 3, 3\n",
    "\n",
    "fig, axes = plt.subplots(nrows=n_sample_viz, ncols=n_images, figsize=(9.0, 9.0))\n",
    "\n",
    "for sample_idx in range(n_sample_viz):\n",
    "    for im_idx in range(n_images):\n",
    "        axes[sample_idx, im_idx].imshow(X_train_data[im_idx][sample_idx][:, :, 0], cmap='Greys')\n",
    "        axes[sample_idx, im_idx].axis('off')\n",
    "        if im_idx==0:\n",
    "            axes[sample_idx, 0].set_title('               Sum value for this row is {}'.format(y_train_data[sample_idx]), \n",
    "                                    fontsize=15, loc='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 77s 1ms/sample - loss: 1.0678\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 70s 1ms/sample - loss: 0.5569\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 71s 1ms/sample - loss: 0.4641\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc6a44f0da0>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, define the vision modules\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Add\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "filters = 64\n",
    "kernel_size = 3\n",
    "\n",
    "import tensorflow as tf\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = [np.expand_dims(t, axis=2) for t in x_train]\n",
    "x_test = [np.expand_dims(t, axis=2) for t in x_test]\n",
    "\n",
    "input_image = Input(shape=(28, 28, 1))\n",
    "\n",
    "y = Conv2D(32, kernel_size=(3, 3),\n",
    "           activation='relu',\n",
    "           input_shape=input_shape)(input_image)\n",
    "y = Conv2D(64, (3, 3), activation='relu')(y)\n",
    "y = MaxPooling2D(pool_size=(2, 2))(y)\n",
    "y = Dropout(0.25)(y)\n",
    "y = Flatten()(y)\n",
    "y = Dense(32, activation='relu')(y)\n",
    "y = Dense(16, activation='relu')(y)\n",
    "output_vec = Dense(1)(y)\n",
    "\n",
    "vision_model = Model(input_image, output_vec)\n",
    "vision_model.compile(loss='mae')\n",
    "vision_model.fit(np.array(x_train), np.array(y_train), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_model.save('vision_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 3, 1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.random.uniform(1, 100, (10, 3))\n",
    "x = np.expand_dims(x, axis=2)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
